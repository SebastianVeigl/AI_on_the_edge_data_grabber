{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training\n",
    "\n",
    "Target of this code is to train a CNN network to classify images of a digital readout to the digits 0 to 9. Additionally a category \"NaN\" is introduced, to mark images that are not amibiguous.\n",
    "\n",
    "### Preparing the training\n",
    "* First all libraries are loaded\n",
    "    * It is assumed, that they are installed during the Python setup\n",
    "* matplotlib is set to print the output inline in the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:27:25.182977Z",
     "start_time": "2024-01-04T14:27:25.156978600Z"
    }
   },
   "outputs": [],
   "source": [
    "########### Basic Parameters for Running: ################################\n",
    "    \n",
    "tflite_model_name_version = \"7segTest\"   # Used for tflite Filename\n",
    "train_val_split = 0.3                   # 0.0 = Use all Images for Training\n",
    "epochs = 200\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image \n",
    "from pathlib import Path\n",
    "\n",
    "total_loss = np.array([])\n",
    "total_val_loss = np.array([])\n",
    "\n",
    "total_accuracy = np.array([])\n",
    "total_val_accuracy = np.array([])\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "* The data is expected in the \"Input_dir\"\n",
    "* Inside files are expected from NaN, 0, 1, ... 9 in which the pictures are sorted according to their values (=category)\n",
    "* Picture size must be 20x32 with 3 color channels (RGB)\n",
    "* The filename after the correct digit at index 0 can be arbitrary\n",
    "\n",
    "* The images are stored in x_data\n",
    "* The expected category for each image is stored in y_data\n",
    "\n",
    "* The last step is a shuffle (from sklearn.utils) and split the data into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T14:27:28.115899300Z",
     "start_time": "2024-01-04T14:27:25.188014200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9574, 32, 20, 3)\n",
      "(9574, 11)\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'digits_resized'\n",
    "\n",
    "files = glob.glob(input_dir + '/*.jpg')\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for cur_file in files:\n",
    "    base = os.path.basename(cur_file)\n",
    "    target = base[0:1]\n",
    "    if target == \"N\":\n",
    "        category = 10                \n",
    "    else:\n",
    "        category = int(target)\n",
    "    test_image = Image.open(cur_file)\n",
    "    test_image = np.array(test_image, dtype=\"float32\")\n",
    "    x_data.append(test_image)\n",
    "    y_data.append(np.array([category]))\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "y_data = to_categorical(y_data, 11)\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "\n",
    "x_data, y_data = shuffle(x_data, y_data)\n",
    "\n",
    "if train_val_split > 0.0:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, test_size=train_val_split)\n",
    "else:\n",
    "    X_train = x_data\n",
    "    y_train = y_data\n",
    "    \n",
    "    X_val = np.array([])\n",
    "    y_val = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "The layout of the network ist a typcial CNN network with alternating **Conv2D** and **MaxPool2D** layers. Finished after **flattening** with additional **Dense** layer.\n",
    "\n",
    "#### Important\n",
    "* Shape of the input layer: (32, 20, 3)\n",
    "* Number of output layers: 11\n",
    "* As loss function \"categorical_crossentropy\" is choosen, as it is a categories task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_1 (Batc  (None, 32, 20, 3)        12        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 20, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 10, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 10, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 8, 5, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 5, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 2, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 11)                1419      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,719\n",
      "Trainable params: 53,713\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=(32, 20, 3)), \n",
    "                             tf.keras.layers.BatchNormalization(), \n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation=\"relu\"), \n",
    "                             tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation=\"relu\"),\n",
    "                             tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation=\"relu\"),\n",
    "                             tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "                             tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.Dense(128,activation=\"relu\"),\n",
    "                             tf.keras.layers.Dropout(0.2),\n",
    "                             tf.keras.layers.Dense(11, activation='softmax')])\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95), metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:27:28.256391800Z",
     "start_time": "2024-01-04T14:27:28.118406700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "The input pictures are randomly scattered for brightness, pixel shift variations and rotation angle. This is implemented with a ImageDataGenerator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-04T14:27:28.264389900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2388/2394 [============================>.] - ETA: 0s - loss: 0.4072 - accuracy: 0.8641\n",
      "Epoch 1: val_accuracy improved from -inf to 0.94222, saving model to checkpoint\\best.ckpt\n",
      "2394/2394 [==============================] - 19s 8ms/step - loss: 0.4065 - accuracy: 0.8643 - val_loss: 0.1871 - val_accuracy: 0.9422\n",
      "Epoch 2/200\n",
      "2393/2394 [============================>.] - ETA: 0s - loss: 0.1577 - accuracy: 0.9476\n",
      "Epoch 2: val_accuracy improved from 0.94222 to 0.95997, saving model to checkpoint\\best.ckpt\n",
      "2394/2394 [==============================] - 16s 7ms/step - loss: 0.1577 - accuracy: 0.9477 - val_loss: 0.1392 - val_accuracy: 0.9600\n",
      "Epoch 3/200\n",
      "2394/2394 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.9638\n",
      "Epoch 3: val_accuracy improved from 0.95997 to 0.97181, saving model to checkpoint\\best.ckpt\n",
      "2394/2394 [==============================] - 34s 14ms/step - loss: 0.1206 - accuracy: 0.9638 - val_loss: 0.1035 - val_accuracy: 0.9718\n",
      "Epoch 4/200\n",
      "2394/2394 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9672\n",
      "Epoch 4: val_accuracy improved from 0.97181 to 0.97842, saving model to checkpoint\\best.ckpt\n",
      "2394/2394 [==============================] - 33s 14ms/step - loss: 0.1075 - accuracy: 0.9672 - val_loss: 0.0743 - val_accuracy: 0.9784\n",
      "Epoch 5/200\n",
      "2391/2394 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9701\n",
      "Epoch 5: val_accuracy did not improve from 0.97842\n",
      "2394/2394 [==============================] - 25s 10ms/step - loss: 0.0999 - accuracy: 0.9700 - val_loss: 0.1050 - val_accuracy: 0.9641\n",
      "Epoch 6/200\n",
      "2392/2394 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9697\n",
      "Epoch 6: val_accuracy did not improve from 0.97842\n",
      "2394/2394 [==============================] - 25s 10ms/step - loss: 0.0966 - accuracy: 0.9697 - val_loss: 0.0776 - val_accuracy: 0.9746\n",
      "Epoch 7/200\n",
      "2393/2394 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9714\n",
      "Epoch 7: val_accuracy did not improve from 0.97842\n",
      "2394/2394 [==============================] - 25s 11ms/step - loss: 0.0890 - accuracy: 0.9714 - val_loss: 0.0778 - val_accuracy: 0.9735\n",
      "Epoch 8/200\n",
      "2388/2394 [============================>.] - ETA: 0s - loss: 0.0862 - accuracy: 0.9749\n",
      "Epoch 8: val_accuracy did not improve from 0.97842\n",
      "2394/2394 [==============================] - 14s 6ms/step - loss: 0.0861 - accuracy: 0.9748 - val_loss: 0.0664 - val_accuracy: 0.9781\n",
      "Epoch 9/200\n",
      "2394/2394 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9732\n",
      "Epoch 9: val_accuracy did not improve from 0.97842\n",
      "2394/2394 [==============================] - 25s 10ms/step - loss: 0.0848 - accuracy: 0.9732 - val_loss: 0.0806 - val_accuracy: 0.9774\n",
      "Epoch 10/200\n",
      "2394/2394 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9763\n",
      "Epoch 10: val_accuracy did not improve from 0.97842\n",
      "2394/2394 [==============================] - 15s 6ms/step - loss: 0.0805 - accuracy: 0.9763 - val_loss: 0.0865 - val_accuracy: 0.9760\n",
      "Epoch 11/200\n",
      "2389/2394 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.9760\n",
      "Epoch 11: val_accuracy did not improve from 0.97842\n",
      "2394/2394 [==============================] - 15s 6ms/step - loss: 0.0801 - accuracy: 0.9760 - val_loss: 0.1001 - val_accuracy: 0.9648\n",
      "Epoch 12/200\n",
      "2386/2394 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9750\n",
      "Epoch 12: val_accuracy did not improve from 0.97842\n",
      "2394/2394 [==============================] - 16s 7ms/step - loss: 0.0869 - accuracy: 0.9749 - val_loss: 0.0967 - val_accuracy: 0.9735\n",
      "Epoch 13/200\n",
      "2384/2394 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9759\n",
      "Epoch 13: val_accuracy improved from 0.97842 to 0.98051, saving model to checkpoint\\best.ckpt\n",
      "2394/2394 [==============================] - 11s 5ms/step - loss: 0.0790 - accuracy: 0.9758 - val_loss: 0.0781 - val_accuracy: 0.9805\n",
      "Epoch 14/200\n",
      "2388/2394 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9776\n",
      "Epoch 14: val_accuracy improved from 0.98051 to 0.98120, saving model to checkpoint\\best.ckpt\n",
      "2394/2394 [==============================] - 12s 5ms/step - loss: 0.0766 - accuracy: 0.9776 - val_loss: 0.0750 - val_accuracy: 0.9812\n",
      "Epoch 15/200\n",
      "2386/2394 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9772\n",
      "Epoch 15: val_accuracy did not improve from 0.98120\n",
      "2394/2394 [==============================] - 12s 5ms/step - loss: 0.0758 - accuracy: 0.9772 - val_loss: 0.1133 - val_accuracy: 0.9742\n",
      "Epoch 16/200\n",
      "2388/2394 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9755\n",
      "Epoch 16: val_accuracy did not improve from 0.98120\n",
      "2394/2394 [==============================] - 12s 5ms/step - loss: 0.0800 - accuracy: 0.9755 - val_loss: 0.0623 - val_accuracy: 0.9798\n",
      "Epoch 17/200\n",
      "2385/2394 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9777\n",
      "Epoch 17: val_accuracy improved from 0.98120 to 0.98364, saving model to checkpoint\\best.ckpt\n",
      "2394/2394 [==============================] - 13s 5ms/step - loss: 0.0784 - accuracy: 0.9775 - val_loss: 0.0444 - val_accuracy: 0.9836\n",
      "Epoch 18/200\n",
      "2385/2394 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9748\n",
      "Epoch 18: val_accuracy improved from 0.98364 to 0.98399, saving model to checkpoint\\best.ckpt\n",
      "2394/2394 [==============================] - 12s 5ms/step - loss: 0.0824 - accuracy: 0.9748 - val_loss: 0.0562 - val_accuracy: 0.9840\n",
      "Epoch 19/200\n",
      "2385/2394 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9765\n",
      "Epoch 19: val_accuracy did not improve from 0.98399\n",
      "2394/2394 [==============================] - 13s 5ms/step - loss: 0.0812 - accuracy: 0.9766 - val_loss: 0.0570 - val_accuracy: 0.9822\n",
      "Epoch 20/200\n",
      "2391/2394 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9779\n",
      "Epoch 20: val_accuracy did not improve from 0.98399\n",
      "2394/2394 [==============================] - 16s 7ms/step - loss: 0.0786 - accuracy: 0.9780 - val_loss: 0.0452 - val_accuracy: 0.9836\n",
      "Epoch 21/200\n",
      "2385/2394 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9770\n",
      "Epoch 21: val_accuracy did not improve from 0.98399\n",
      "2394/2394 [==============================] - 16s 7ms/step - loss: 0.0889 - accuracy: 0.9770 - val_loss: 0.0713 - val_accuracy: 0.9749\n",
      "Epoch 22/200\n",
      "2391/2394 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9778\n",
      "Epoch 22: val_accuracy did not improve from 0.98399\n",
      "2394/2394 [==============================] - 16s 7ms/step - loss: 0.0822 - accuracy: 0.9779 - val_loss: 0.1039 - val_accuracy: 0.9729\n",
      "Epoch 23/200\n",
      "2394/2394 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9781\n",
      "Epoch 23: val_accuracy did not improve from 0.98399\n",
      "2394/2394 [==============================] - 18s 8ms/step - loss: 0.0747 - accuracy: 0.9781 - val_loss: 0.0593 - val_accuracy: 0.9819\n",
      "Epoch 24/200\n",
      "2392/2394 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9801\n",
      "Epoch 24: val_accuracy did not improve from 0.98399\n",
      "2394/2394 [==============================] - 15s 6ms/step - loss: 0.0739 - accuracy: 0.9802 - val_loss: 0.0549 - val_accuracy: 0.9829\n",
      "Epoch 25/200\n",
      "2390/2394 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9786\n",
      "Epoch 25: val_accuracy did not improve from 0.98399\n",
      "2394/2394 [==============================] - 15s 6ms/step - loss: 0.0841 - accuracy: 0.9786 - val_loss: 0.0810 - val_accuracy: 0.9788\n",
      "Epoch 26/200\n",
      " 277/2394 [==>...........................] - ETA: 10s - loss: 0.1191 - accuracy: 0.9738"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "shift_range = 1\n",
    "brightness_range = 0.3\n",
    "rotation_angle = 10\n",
    "zoom_range = 0.4\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=[-shift_range, shift_range],\n",
    "                             height_shift_range=[-shift_range, shift_range],\n",
    "                             brightness_range=[1 - brightness_range, 1 + brightness_range],\n",
    "                             zoom_range=[1 - zoom_range, 1 + zoom_range],\n",
    "                             rotation_range=rotation_angle)\n",
    "\n",
    "checkpoint_path = 'checkpoint/best.ckpt'\n",
    "save_best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                        save_weights_only=True,\n",
    "                                                        monitor='val_accuracy',\n",
    "                                                        mode='max',\n",
    "                                                        save_best_only=True,\n",
    "                                                        verbose=1)\n",
    "\n",
    "if train_val_split > 0.0:\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "    \n",
    "    train_iterator = datagen.flow(x_data, y_data, batch_size=batch_size)\n",
    "    validation_iterator = datagen.flow(X_val, y_val, batch_size=batch_size)\n",
    "    history = model.fit(train_iterator, validation_data=validation_iterator, epochs=epochs, callbacks=[early_stopping_callback, save_best_callback])\n",
    "else:\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=15)\n",
    "    \n",
    "    train_iterator = datagen.flow(x_data, y_data, batch_size=batch_size)\n",
    "    history = model.fit(train_iterator, epochs=epochs, callbacks=[early_stopping_callback, save_best_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load best weights\n",
    "model.load_weights(checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learing result\n",
    " \n",
    "* Visualization of the training and validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "total_loss = np.append(total_loss, history.history['loss'])\n",
    "plt.semilogy(history.history['loss'])\n",
    "\n",
    "if train_val_split > 0:\n",
    "    total_val_loss = np.append(total_val_loss, history.history['val_loss'])\n",
    "    plt.semilogy(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_accuracy = np.append(total_accuracy, history.history['accuracy'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "if train_val_split > 0:\n",
    "    total_val_accuracy = np.append(total_val_accuracy, history.history['val_accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model by hand\n",
    "\n",
    "* The following code uses the trained model to check the deviation for each picture.\n",
    "* x-axis walks through each pixel, y-axis shows the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check each image for expected and deviation\n",
    "* setting the switch \"only_deviation = true\" will only print the images for which the classification and the CNN-result deviates\n",
    "\n",
    "The output contains the following information:\n",
    "\n",
    "| Filename      | Expected Category           | Predicted Category        |\n",
    "|------------- |:-----------------------------:|--------------|\n",
    "| ziffer_sortiert_resize_NaN/5\\Ziffer_4_0034.jpg | 4  | -1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "only_deviation = True\n",
    "show_wrong_image = True\n",
    "\n",
    "files = glob.glob(input_dir + '/*.jpg')\n",
    "\n",
    "for cur_file in files:\n",
    "    base = os.path.basename(cur_file)\n",
    "    target = base[0:1]\n",
    "    if target == \"N\":\n",
    "        zw1 = -1\n",
    "    else:\n",
    "        zw1 = int(target)\n",
    "    expected_class = zw1\n",
    "    image_in = Image.open(cur_file)\n",
    "    test_image = np.array(image_in, dtype=\"float32\")\n",
    "    img = np.reshape(test_image,[1,32,20,3])\n",
    "    classes = np.argmax(model.predict(img, verbose=0), axis=-1)\n",
    "    classes = classes[0]\n",
    "    if classes == 10: \n",
    "        classes = -1\n",
    "    zw2 = classes\n",
    "    zw3 = zw2 - zw1\n",
    "    res.append(np.array([zw1, zw2, zw3]))\n",
    "    if only_deviation:\n",
    "        if str(classes) != str(expected_class):\n",
    "            print(cur_file + \" \" + str(expected_class) + \" \" + str(classes))\n",
    "            if show_wrong_image:\n",
    "                plt.imshow(image_in)\n",
    "        \n",
    "res = np.asarray(res)\n",
    "\n",
    "plt.plot(res[:,0])\n",
    "plt.plot(res[:,1])\n",
    "plt.title('Result')\n",
    "plt.ylabel('Digital Value')\n",
    "plt.xlabel('#Picture')\n",
    "plt.legend(['real','model'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "* Save the model to the file with the \"h5\" file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(f'../models/{tflite_model_name_version}.keras')\n",
    "Path(f'../models/{tflite_model_name_version}.keras').stat().st_size"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(f'../models/{tflite_model_name_version}.tflite', 'wb') as h5_file:\n",
    "    h5_file.write(tflite_model)\n",
    "    \n",
    "Path(f'../models/{tflite_model_name_version}.tflite').stat().st_size"
   ],
   "metadata": {
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "filename = f'../models/{tflite_model_name_version}q.tflite'\n",
    "\n",
    "def representative_dataset():\n",
    "    for n in range(x_data[0].size):\n",
    "      data = np.expand_dims(x_data[5], axis=0)\n",
    "      yield [data.astype(np.float32)]\n",
    "        \n",
    "converter2 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter2.representative_dataset = representative_dataset\n",
    "converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter2.representative_dataset = representative_dataset\n",
    "tflite_quant_model = converter2.convert()\n",
    "\n",
    "with open(filename, 'wb') as q_file:\n",
    "    q_file.write(tflite_quant_model)\n",
    "\n",
    "Path(filename).stat().st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
